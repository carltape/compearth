%  dvips -t letter specfem.dvi -o specfem.ps ; ps2pdf specfem.ps
%  pdflatex specfem.tex

\documentclass[10pt,fleqn,letterpaper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[round]{natbib}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{xcolor,listings}

%\usepackage{fancyhdr}
%\pagestyle{fancy}

%=====================================================
%       SPACING COMMANDS (Latex Companion, p. 52)
%=====================================================

\usepackage{setspace}

\usepackage[margin=1in]{geometry}

%\include{NEWCOMMANDS}
\newcommand{\specfem}{\textsc{Specfem3D}}
\newcommand{\cubit}{\textsc{Cubit}}
\newcommand{\geocubit}{\textsc{GeoCubit}}


\graphicspath{
  {figures/}
}

\title{\specfem\ Tutorial for 2013 CIG-QUEST-IRIS Workshop}
\author{Carl, Qinya, Elliott, Emanuele}
\date{Last compiled: \today}

\usepackage[bookmarks=true,colorlinks=true,%
            pdfusetitle=true%
           ]{hyperref}

\definecolor{codebg}{RGB}{238,238,238}
\definecolor{codeframe}{RGB}{204,204,204}
\lstset{
  language=bash,
  basicstyle=\small\ttfamily,
  columns=fullflexible,
  keepspaces=true,
  breakatwhitespace=true,
  breaklines=true,
  prebreak=\raisebox{0ex}[0ex][0ex]{%
    \ensuremath{\rightharpoondown}},
  postbreak=\raisebox{0ex}[0ex][0ex]{%
    \ensuremath{\hookrightarrow\space}},
  escapechar=\@,
  backgroundcolor=\color{codebg},
  frame=single,
  framesep=10pt,
  rulecolor=\color{codeframe}
}

%=====================================================
\begin{document}
%=====================================================

\maketitle

%=====================================================

%\tableofcontents

\subsection*{Overview}

%--------------

\subsection*{Document Conventions}

\begin{enumerate}
\item In this document, commands you should run in your shell or code blocks
      you should edit are indicated by an framed grey box:
\begin{lstlisting}
This is a code block.
\end{lstlisting}

\item Things you should type at the prompt are prefixed by a dollar sign \$,
      and the output from commands is not. Comments are prefixed by a hash \#.
\begin{lstlisting}
$ echo "A command you can run" | sed 's/can/have/g' # This is a comment.
A command you have run
\end{lstlisting}

\item Lines that have been wrapped are indicated by $\rightharpoondown$ at the
      end of the line and $\hookrightarrow$ at the beginning of the continued
      line. Line breaks do not occur in the middle of words, so remember to
      include a space between each wrapped line if you are typing it out yourself.
\begin{lstlisting}
This is a very long line that will wrap because it is long and also a run-on sentence, too.
\end{lstlisting}
\end{enumerate}

%--------------

\subsection*{Log-in instructions}

\begin{enumerate}
\item get account info from ARSC
\item ssh into ARSC (is X11 okay from mac? -X?)
\item Explain text editing (vi, gedit, emacs)
\end{enumerate}

%---------------

\subsection*{Step-by-step instructions, example 1: homogeneous halfspace synthetic seismograms (better call it forward simulation?)}

The purpose of this example is to step through all the key steps of \specfem: get the code, configure the code for your cluster, generate a mesh using (\textsc{Geo})\cubit, partition the mesh, generate databases, run the solver, and check the output. The example is trivial, but it is important to remember that 

\begin{enumerate}
\item check what modules are loaded by default
\begin{lstlisting}
$ module list
Currently Loaded Modulefiles:
  1) pgi/13.4                 3) netcdf/4.3.0.pgi-13.4
  2) openmpi-pgi-13.4/1.4.3   4) PrgEnv-pgi/13.4
\end{lstlisting}

\item Check out two copies\footnote{There's no need for two copies, but they will make it a bit easier to keep track of the different exampled.} of \specfem\ version 22719 from the code repository at CIG (\url{http://www.geodynamics.org/})
%
\begin{lstlisting}
$ svn co -r 22719 http://geodynamics.org/svn/cig/seismo/3D/SPECFEM3D/trunk SPECFEM3D_22719_default
$ svn co -r 22719 http://geodynamics.org/svn/cig/seismo/3D/SPECFEM3D/trunk SPECFEM3D_22719_socal
\end{lstlisting}
%
(This may take a few minutes.)

\item check version info
%
\begin{lstlisting}
$ cd SPECFEM3D_22719_default
$ svn info
Path: .
URL: http://geodynamics.org/svn/cig/seismo/3D/SPECFEM3D/trunk
Repository Root: http://geodynamics.org/svn/cig
Repository UUID: 42e91aa9-f6fe-0310-9bd8-dd834c9eb30a
Revision: 22728
Node Kind: directory
Schedule: normal
Last Changed Author: danielpeter
Last Changed Rev: 22719
Last Changed Date: 2013-08-20 06:16:30 -0800 (Tue, 20 Aug 2013)
\end{lstlisting}

The {\em base directory} for the code is \verb+SPECFEM3D_22719_default+, which we will call \verb+SPECFEM3D+ in the instructions.

\item Take a quick look at the user manual, mainly as a reminder to come back to it for details. 
%
\begin{lstlisting}
$ evince doc/USER_MANUAL/manual_SPECFEM3D_Cartesian.pdf &
\end{lstlisting}

\item Check that all software is available (or that modules are loaded):
\begin{lstlisting}
$ which mpirun # OpenMPI
$ which cubit  # Cubit
$ which python # Python
\end{lstlisting}

\item Configure package, e.g. using the portland compiler:
\begin{lstlisting}
$ ./configure F90=pgf90 MPIFC=mpif90
\end{lstlisting}

If successful, this command will generate several \verb+Makefile+s in the \verb+SPECFEM3D+ main directory as well as subdirectories of verb+src/+, as well as \verb+shared/constants.h+ and \verb+shared/precision.h+, among others.

\item Adapt the run scripts for your cluster and your example run.

- copy run scripts from \verb+utils/Cluster/+ into \verb+SPECFEM3D/+, e.g.,
\begin{lstlisting}
$ cd utils/Cluster/pbs/
$ cp go_decomposer_pbs.bash go_generate_databases_pbs.bash go_solver_pbs.bash ../../../
$ cd ../../../
\end{lstlisting}

Set all bash scripts to run in the \verb+standard+ queue (in general, the queue will depend on your specific cluster) by editing the line
\textcolor{blue}{(do we need to warn people to also delete one \# from the line of \#\#PBS -q debug?)}
\begin{lstlisting}
#PBS -q standard
\end{lstlisting}
(The default simulation time and number of cores is okay for this example.)
 
\item The rest of the instructions follow from the homogeneous halfspace example here
\begin{lstlisting}
SPECFEM3D/examples/homogeneous_halfspace/README
\end{lstlisting}

Copy the input files from examples directory into \verb+SPECFEM3D/DATA/+
\begin{lstlisting}
$ cd examples/homogeneous_halfspace/DATA/
$ cp * ../../../DATA/
\end{lstlisting}
%
Note that only three input files are required: for the source (\verb+CMTSOLUTION+), for the stations (\verb+STATIONS+), and for the simulation parameters (\verb+Par_file+).

\item create mesh:

From the directory \verb+SPECFEM3D/examples/homogeneous_halfspace+, open the \cubit\ GUI, \verb+claro+:
\begin{lstlisting}
$ claro
\end{lstlisting}
\textcolor{blue}{maybe warn people that the X11 forwarding can be slow?}
%
(Close the ``Tip of the Day''.)
To ensure that the path is local and the needed python modules and scripts are accessible, \verb+File --> Set Directory+, then click \verb+Choose+  \textcolor{blue}{(Choose what? examples/homogenous\_halfspace/?)}, which is equivalent to typing in the \cubit\ command window \verb+cd 'examples/homogeneous_halfspace/'+ (note the single quotes are required).

Run the meshing script: from the Menu bar, select \verb+Tools -> Play Journal File+, set \verb+Files of Type+ to \verb+All Files+, then select \verb+block_mesh.py+. 


\textcolor{blue}{I can't seem to get this working!}

If everything goes fine, this script creates the ten mesh files in subdirectory \verb+MESH/+:
\begin{lstlisting}
$ ls MESH
MESH/absorbing_surface_file_bottom
MESH/absorbing_surface_file_xmax
MESH/absorbing_surface_file_xmin
MESH/absorbing_surface_file_ymax
MESH/absorbing_surface_file_ymin
MESH/free_surface_file
MESH/materials_file
MESH/mesh_file
MESH/nodes_coords_file
MESH/nummaterial_velocity_file
\end{lstlisting}

You should be able to translate, rotate, and zoom on the mesh using a three-button mouse. (This can be emulated if you set X11 preferences, then (on a Mac) hold the \verb+control+, \verb+alt+, or \verb+command+ buttons while clicking and moving the mouse.)

The \cubit\ graphics window should show a mesh similar to the file
\begin{verbatim}
picture_of_this_homogeneous_regular_mesh.png
\end{verbatim}

\item decompose mesh files:

Compile the decomposer in directory \verb+SPECFEM3D/+:
\begin{lstlisting}
$ make xdecompose_mesh
\end{lstlisting}
%
This will compile the partitioner \textsc{Scotch}.

Then run the decomposer:
\begin{lstlisting}
$ qsub go_decomposer_pbs.bash
\end{lstlisting}
%
You can check the status of the job with the command
%
\begin{lstlisting}
$ qmap | grep USERNAME
\end{lstlisting}
%
The job should take 20 seconds or so. It creates the four mesh partitions \verb+proc000***_Database+ in the directory \verb+OUTPUT_FILES/DATABASES_MPI/+. The output file \verb+OUTPUT_FILES/*.o+ contains information on the partitioning.

\item generate databases:

Compile \verb+generate_databases+ in the directory \verb+SPECFEM3D/+:
\begin{lstlisting}
$ make xgenerate_databases
\end{lstlisting}
Submit the job:
\begin{lstlisting}
$ qsub go_generate_databases_pbs.bash
\end{lstlisting}

The job should take about a minute.
It creates binary mesh files, e.g. \verb+proc000***_external_mesh.bin+ in the directory \verb+OUTPUT_FILES/DATABASES_MPI/+.

It is a good idea to look at the partitions of the mesh files. Load some vtk files (e.g., vs) into paraview:
\begin{lstlisting}
$ cd OUTPUT_FILES/DATABASES_MPI/
$ module load paraview
$ paraview
\end{lstlisting}
%
Then \verb+File --> Open+, and select all four \verb+vs*vtk+ files. When you are done, be sure to unload the \verb+paraview+ module, since here it was compiled with \verb+gnu+, which conflicts with the \verb+portland+ we are using.
%
\begin{lstlisting}
$ module unload paraview
$ cd ../../
\end{lstlisting}

\item run simulation:

   - compile specfem3D (from \verb+SPECFEM3D/+):
\begin{lstlisting}
$ make xspecfem3D
\end{lstlisting}
   - submit script to run solver:
\begin{lstlisting}
$ qsub go_solver_pbs.bash
\end{lstlisting}

The simulation runs on 4 cores and should take about 30 minutes. You can track the progress with the timestamp files generated in \verb+OUTPUT_FILES/+ (type \verb+ls -ltr+ to see the most recent files). When the job is complete, you should have 3 sets (semd,semv,sema) of 12 (\verb+ls -1 *semd | wc+) seismogram files in the directory \verb+OUTPUT_FILES+, as well as 51 \verb+timestamp******+ files.

\item Compare your computed seismograms with the reference seismograms.

A quick visual comparison can be done from \verb+SPECFEM3D/+ using \verb+xmgrace+:
\begin{lstlisting}
$ module load grace
$ xmgrace examples/homogeneous_halfspace/REF_SEIS/*Z.semd &
$ xmgrace OUTPUT_FILES/*Z.semd &
\end{lstlisting}

\end{enumerate}

%===============================================================================

\subsection*{Step-by-step instructions, example 2: homogeneous halfspace sensitivity kernel}

\begin{enumerate}
\item QINYA'S INSTRUCTIONS HERE -- note that there are some new scripts added (by Daniel?)
\end{enumerate}

%===============================================================================

\subsection*{Step-by-step instructions, example 3: southern California}

This example is at a scale that will likely not run on a single laptop or desktop computer. In other words, the required memory must be distributed over a number of different machines in order to run the simulation. Here we use 96 cores of the cluster.

\begin{enumerate}
\item Copy bash scripts into the base directory:

\begin{lstlisting}
$ cd SPECFEM3D_22719_socal
$ cp ../SPECFEM3D_22719_default/*bash .
\end{lstlisting}

\item configure
\begin{lstlisting}
$ ./configure F90=pgf90 MPIFC=mpif90
\end{lstlisting}

\item compile all
\begin{lstlisting}
$ make all
\end{lstlisting}

\item Link the mesh directory as an example
\begin{lstlisting}
$ cd examples
$ ln -s /import/c/d/ERTHQUAK/GEOCUBIT_MESH/socal_med400km .
\end{lstlisting}

\item Modify \verb+go_decomposer_pbs.bash+ to point to the new directory:
\begin{lstlisting}
MESHDIR=examples/socal_med400km/MESH/
\end{lstlisting}

\item Link the tomography file and copy input files
\begin{lstlisting}
$ cd DATA
$ ln -s /import/c/d/ERTHQUAK/MODEL/cvm119_1000_1000_0250_741_549.xyz tomography_model.xyz
$ cp ../examples/socal_med400km/in_data_files/* .
\end{lstlisting}

\item modify \verb+go_generate_databases_pbs.bash+ and \verb+go_solver_pbs.bash+ to have the proper time limits and number of cores
\begin{lstlisting}
#PBS -l nodes=6:ppn=16,walltime=1:00:00
#PBS -q standard
\end{lstlisting}

%\item delete the empty directory
%
%\begin{lstlisting}
%rm -rf OUTPUT_FILES/DATABASES_MPI
%\end{lstlisting}
%
%This simulation will create a directory \verb+DATABASES_socal_med400km+ that is outside of \verb+OUTPUT_FILES+. This is useful% since we may have many simulations associated with the same mesh.

\item Follow the same steps as in Example 1: decompose, generate databases, solver. Here the programs have all been compiled, so only submitting the run scripts is needed. (But wait for each one to finish, and check the output before proceeding to the next step.)
%
\begin{itemize}
\item The decomposer takes about 3 minutes.
\item The generate databases takes about 15 minutes.
\item The solver takes about 25 minutes.
\end{itemize}
%
The simulation runs on 96 cores and should take about 25 minutes. You can track the progress with the timestamp files generated in \verb+OUTPUT_FILES/+ (type \verb+ls -ltr+ to see the most recent files). When the job is complete, you should have 3 sets (semd,semv,sema) of 1107 (\verb+ls -1 *semd | wc+) seismogram files in the directory \verb+OUTPUT_FILES+, as well as 4 \verb+timestamp******+ files.

As expected, the seismograms contain numerical noise, since the source half duration in \verb+CMTSOLUTION+ was set to 0~s. This allows for maximal flexibility in post-processing, since the seismograms can be convolved with any source time function (see manual). However, it is important to know what the minimum resolving period of a particular mesh and model is, since these periods provide a guide for how to filter the seismograms in post-processing.

\item Now make a change to one of the input files in \verb+SPECFEM3D/DATA/+, either \verb+Par_file+, \verb+CMTSOLUTION+, or \verb+STATIONS+. Rename the \verb+OUTPUT_FILES+ directory if you do not want to over-write your previous output. Then submit the new job. (Note that no recompilation is needed.)

\end{enumerate}

%===============================================================================

\subsection*{Step-by-step instructions, example 4: GPU}

In this example, we show how \specfem\ can be used on a GPU cluster. In general,
there are only two {\em required} steps to use GPU computing:

\begin{enumerate}
\item Enable CUDA during configuration:
\begin{lstlisting}
$ ./configure --enable-cuda MPI_INC=-I$MPI_DIR/include
\end{lstlisting}

\item Enable CUDA at runtime in \verb|DATA/Par_file|:
\begin{lstlisting}
GPU_MODE    =   .true.
\end{lstlisting}
\end{enumerate}

In practice, you may need to do several other steps depending on how your
cluster has been set up. When running on your own cluster, you should consult
their documentation in case there are additional steps that must be taken. The
following example shows the extra steps that are required on the FISH cluster
at ARSC.

\begin{enumerate}
\item *TODO*: start with login to fish

\item Check out version 22719 from SVN again:
\begin{lstlisting}
$ svn co -r 22719 http://geodynamics.org/svn/cig/seismo/3D/SPECFEM3D/trunk SPECFEM3D_22719_GPU
$ cd SPECFEM3D_22719_GPU
\end{lstlisting}

\item Load the CUDA toolkit:
\begin{lstlisting}
$ module load cudatoolkit
\end{lstlisting}

\item The compilers on fish work a little differently than on pacman. There are
      wrappers that automatically run the real compilers and apply MPI or CUDA
      options once the modules are loaded.

      When compiling for CUDA, the CUDA compiler (which is separate from the C
      or Fortran compilers) needs to know where the MPI headers are installed.
      This information must be set using the \verb|MPI_INC| variable when
      running \verb|./configure|. On fish, loading the MPI module sets the
      \verb|MPICH_DIR| environment variable, which is used in the instructions
      below.

\begin{lstlisting}
$ ./configure MPIFC=ftn CC=cc FC=ftn --with-cuda MPI_INC=-I$MPICH_DIR/include
\end{lstlisting}

\item *TODO*: Copy example config stuff...

\item The GPU mode only affects the solver (\verb|xspecfem3D|), but it is a good
      idea to remember to enable it now. Edit \verb|DATA/Par_file| and set this
      option:
\begin{lstlisting}
GPU_MODE    =   .true.
\end{lstlisting}

\item Now, you can compile things as usual:
\begin{lstlisting}
$ make xdecompose_mesh
$ make xgenerate_databases
$ make xspecfem3D
\end{lstlisting}

\item The scripts to run jobs on fish are similar to the ones on pacman. Fish
      uses a slightly different method to run MPI programs compared to pacman.
      Instead of using \verb|mpirun -np <#procs>|, you use
      \verb|aprun -n <#procs>|, and to use GPUs, you must use the \verb|gpu|
      queue.
\begin{lstlisting}
$ cd utils/Cluster/pbs/
$ cp go_decomposer_pbs.bash go_generate_databases_pbs.bash go_solver_pbs.bash ../../../
$ cd ../../../
$ vim *.bash
  # Change 'mpirun -np' to 'aprun -n'
  # Add '#PBS -q gpu'
\end{lstlisting}
*TODO*: Add note about one process per node for GPU runs. This probably depends
on which example we're using.

\item Now, you can submit your jobs as usual:
\begin{lstlisting}
$ qsub go_decomposer_pbs.bash
# Wait for it to finish

$ qsub go_generate_databases_pbs.bash
# Wait for it to finish

$ qsub go_solver_pbs.bash
# Wait for it to finish
\end{lstlisting}

\item Depending on how many GPUs you use, you may see up to 20 or 30 times
      faster execution. If everything ran correctly on the GPU, you should see
      some additional files in \verb|OUTPUT_FILES|:
\begin{lstlisting}
$ ls OUTPUT_FILES/gpu*.txt
gpu_device_info_proc_000000.txt
gpu_device_mem_usage_proc_000000.txt
gpu_device_info_proc_000001.txt
gpu_device_mem_usage_proc_000001.txt
gpu_device_info_proc_000002.txt
gpu_device_mem_usage_proc_000002.txt
gpu_device_info_proc_000003.txt
gpu_device_mem_usage_proc_000003.txt
\end{lstlisting}

      These files contain statistics of the run on the GPU devices. If the jobs
      crash, you should check these files to ensure that you did not use too
      much memory per GPU.
\end{enumerate}

%=====================================================
% REFERENCES

%\begin{spacing}{1.0}
%\bibliographystyle{agu08}
%\bibliography{preamble,REFERENCES,refs_carl,refs_socal,refs_source}
%\end{spacing}

%=====================================================
\end{document}
%=====================================================
